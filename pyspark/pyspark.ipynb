{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, download the data from the eCommerce Events History in Cosmetics Shop kaggle repository: https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop?resource=download&select=2020-Jan.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- DataScience con PySpark I: Apache Spark, Python, DataFrames y RDDs:\n",
    "https://www.youtube.com/watch?v=iMOgTbaDJXc\n",
    "\n",
    "- Pandas vs PySpark DataFrame With Examples:\n",
    "https://sparkbyexamples.com/pyspark/pandas-vs-pyspark-dataframe-with-examples/#:~:text=Copy-,What%20is%20PySpark%3F,(100x)%20faster%20than%20Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Pandas?\n",
    "\n",
    "Pandas is one of the most used open-source Python libraries to work with Structured tabular data for analysis. Pandas library is heavily used for Data Analytics, Machine learning, data science projects, and many more. Pandas can load the data by reading CSV, JSON, SQL, many other formats and creates a DataFrame which is a structured object containing rows and columns (similar to SQL table).\n",
    "\n",
    "It doesn’t support distributed processing (use of more than one processor to perform the processing for an individual task.) hence you would always need to increase the resources when you need additional horsepower to support your growing data. However, there is Moding which is an open-source Python library that accelerated Pandas workflow by distributing operations across multiple cores of the CPU. Unlike other distributed libraries, Modin can be easily integrated and compatible with Pandas library and has similar APIs. As a renference, here is a good link for anyone interested: https://towardsdatascience.com/speed-up-your-pandas-workflow-by-changing-a-single-line-of-code-11dfd85efcfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is a Spark library written in Python to run Python applications using Apache Spark capabilities. Using PySpark we can run applications parallelly on the distributed cluster (multiple nodes) or even on a single node. In other words, PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most of Spark’s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.\n",
    "\n",
    "Spark basically written in Scala and later on due to its industry adaptation it’s API PySpark released for Python using Py4J. Py4J is a Java library that is integrated within PySpark and allows python to dynamically interface with JVM objects, hence to run PySpark you also need Java to be installed along with Python, and Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark Features\n",
    "\n",
    "- In-memory computation\n",
    "- Distributed processing using parallelize\n",
    "- Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)\n",
    "- Fault-tolerant\n",
    "- Immutable\n",
    "- Lazy evaluation\n",
    "- Cache & persistence\n",
    "- Inbuild-optimization when using DataFrames\n",
    "- Supports ANSI SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark Advantages\n",
    "\n",
    "- PySpark is a general-purpose, in-memory, distributed processing engine that allows you to process data efficiently in a distributed fashion.\n",
    "- Applications running on PySpark are 100x faster than traditional systems.\n",
    "- You will get great benefits from using PySpark for data ingestion pipelines.\n",
    "- Using PySpark we can process data from Hadoop HDFS, AWS S3, and many file systems.\n",
    "- PySpark also is used to process real-time data using Streaming and Kafka.\n",
    "- Using PySpark streaming you can also stream files from the file system and also stream from the socket.\n",
    "- PySpark natively has machine learning and graph libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark Modules & Packages\n",
    "\n",
    "- PySpark RDD (pyspark.RDD)\n",
    "    1. Se procesan sobre distintas máquinas\n",
    "    2. Sí son tolerantes a fallos\n",
    "    3. Son inmutables. Si quieres modificarlo te creas otro RDD con el resultado\n",
    "- PySpark DataFrame and SQL (pyspark.sql)\n",
    "    1. Se procesan siempre sobre una única máquina\n",
    "    2. No son tolerantes a fallos\n",
    "    3. Sin mutables\n",
    "- PySpark Streaming (pyspark.streaming)\n",
    "- PySpark MLib (pyspark.ml, pyspark.mllib)\n",
    "- PySpark GraphFrames (GraphFrames)\n",
    "- PySpark Resource (pyspark.resource) It’s new in PySpark 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between Pandas and PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very simple words Pandas run operations on a single machine whereas PySpark runs on multiple machines. If you are working on a Machine Learning application where you are dealing with larger datasets, PySpark is a best fit which could processes operations many times(100x) faster than Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Decide Between Pandas vs PySpark\n",
    "\n",
    "If the data is small enough that you can use pandas to process it, then you likely don't need pyspark. Spark is useful when you have such large data sizes that it doesn't fit into memory in one machine since it can perform distributed computation. That being said, if the computation is complex enough that it could benefit from a lot of parallelization, then you could see an efficiency boost using pyspark. I'm more comfortable with pyspark's APIs than pandas, so I might end up using pyspark anyways, but whether you'll see an efficiency boost depends a lot on the problem.\n",
    "\n",
    "Below are the few considerations when to choose PySpark over Pandas\n",
    "\n",
    "- If your data is huge and grows significantly over the years and you wanted to improve your processing time.\n",
    "- If you want fault-tolerant.\n",
    "- ANSI SQL compatibility.\n",
    "- Language to choose (Spark supports Python, Scala, Java & R)\n",
    "- When you want Machine-learning capability.\n",
    "- Would like to read Parquet, Avro, Hive, Casandra, Snowflake e.t.c\n",
    "- If you wanted to stream the data and process it real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time pandas: 24.84 sec\n",
      "Time spark: 23.91 sec\n"
     ]
    }
   ],
   "source": [
    "# Pandas\n",
    "start = time.time()\n",
    "\n",
    "files_s = glob.glob(\"data/*.csv\")\n",
    "li = []\n",
    "for filename in files_s:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "df_pandas = pd.concat(li, axis=0, ignore_index=True)\n",
    "df_pandas.head(2)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time pandas: {(end - start):.2f} sec')\n",
    "\n",
    "# PySpark\n",
    "start = time.time()\n",
    "\n",
    "df_spark = spark.read.options(header='True', inferSchema='True').csv('data/*.csv')\n",
    "df_spark\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time spark: {(end - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20692840\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20692840 entries, 0 to 20692839\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   event_time     object \n",
      " 1   event_type     object \n",
      " 2   product_id     int64  \n",
      " 3   category_id    int64  \n",
      " 4   category_code  object \n",
      " 5   brand          object \n",
      " 6   price          float64\n",
      " 7   user_id        int64  \n",
      " 8   user_session   object \n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 1.4+ GB\n",
      "None\n",
      "20692840\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Pandas\n",
    "print(len(df_pandas))\n",
    "print(df_pandas.info())\n",
    "\n",
    "# PySpark\n",
    "print(df_spark.count())\n",
    "print(df_spark.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['view' 'cart' 'remove_from_cart' 'purchase']\n",
      "Time pandas: 0.72 sec\n",
      "+----------------+\n",
      "|      event_type|\n",
      "+----------------+\n",
      "|        purchase|\n",
      "|            view|\n",
      "|            cart|\n",
      "|remove_from_cart|\n",
      "+----------------+\n",
      "\n",
      "None\n",
      "Time spark: 5.82 sec\n"
     ]
    }
   ],
   "source": [
    "# Pandas\n",
    "start = time.time()\n",
    "\n",
    "print(df_pandas['event_type'].unique())\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time pandas: {(end - start):.2f} sec')\n",
    "\n",
    "# PySpark\n",
    "start = time.time()\n",
    "\n",
    "print(df_spark.select('event_type').distinct().show())\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time spark: {(end - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the product IDs when the event type is cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5768333\n",
      "Time pandas: 1.48 sec\n",
      "5768333\n",
      "Time spark: 4.56 sec\n"
     ]
    }
   ],
   "source": [
    "# Pandas\n",
    "start = time.time()\n",
    "\n",
    "print(len(df_pandas[df_pandas['event_type'] == 'cart']['product_id']))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time pandas: {(end - start):.2f} sec')\n",
    "\n",
    "# PySpark\n",
    "start = time.time()\n",
    "\n",
    "print(df_spark.select(['product_id']).filter(\"event_type='cart'\").count())\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time spark: {(end - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5768333\n",
      "Time pandas: 1.43 sec\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 5768333|\n",
      "+--------+\n",
      "\n",
      "Time spark: 4.39 sec\n"
     ]
    }
   ],
   "source": [
    "# Pandas\n",
    "start = time.time()\n",
    "\n",
    "print(len(df_pandas[df_pandas['event_type'] == 'cart']['product_id']))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time pandas: {(end - start):.2f} sec')\n",
    "\n",
    "# PySpark\n",
    "start = time.time()\n",
    "\n",
    "# Temp view called data of our dataframe\n",
    "df_spark.createOrReplaceTempView('dataSpark')\n",
    "spark.sql(\"select count(*) from dataSpark where event_type = 'cart'\").show()\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time spark: {(end - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DEAP_main",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
