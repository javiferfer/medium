{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhhWh3soa4y7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our own reviews to demonstrate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spMQl7YP-h7a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review1 = \"Fictional Movie combines aspects of the action, adventure, and suspense genres into a delightful blend. Although the plotline is difficult to describe without spoilers, it delivers on every promise, keeping the audience on the edge of their seats!\"\n",
    "review2 = \"Fictional Movie underwhelms on all accounts. The story is insipid and uninspired. The pacing is sluggish, with long stretches of irrelevant and unimportant segues whose import is at best an inside joke to the writers. You might as well not bother with this movie.\"\n",
    "review3 = \"Some movies have great visuals, clever writing, or heartwarming stories. Fictional Movie has none of these things. It is a pallid imitation of better movies of bygone eras. If you are going to see one movie this year, see something else.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice all the positive labels? We need to shuffle the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imdb_dataset**: This is the dataset we will be working with. It involves reviews and a label of 'positive' or 'negative' associated with the review sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xh3lO2rkcLed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "data_train = imdb_dataset(train=True)\n",
    "data_test = imdb_dataset(test=True)\n",
    "shuffle(data_train)\n",
    "shuffle(data_test)\n",
    "print(len(data_train), len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12436,
     "status": "ok",
     "timestamp": 1573123244269,
     "user": {
      "displayName": "Javier Fernandez",
      "photoUrl": "",
      "userId": "14768449372096605825"
     },
     "user_tz": -540
    },
    "id": "27xS4gARcf1a",
    "outputId": "2eacfb04-aeb1-45d8-fbca-f52d12fa3470",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: neg\n",
      "Text: I suppose if you like endless dialogue that doesn't forward the story and flashy camera effects like the scene transitions in the television show _Angel_, you'll enjoy the film. Me? All I wanted was a nice, tight little story, and it wasn't there. The pacing was practically backward, plot points were buried under a sea of unneeded dialogue, and there was absolutely no sense of dread, or tension, or ANYTHING.<br /><br />Is it the redneck? Is it the Wendigo? No, it's a cameraman on speed. That's not scary. It doesn't generate a single note of tension or atmosphere unless you're scared by MTV. Like those reviewers before me, I too noticed that by the end the movie invokes derisive laughter from the audience.<br /><br />Terrible film.\n",
      "\n",
      "Sentiment: neg\n",
      "Text: I can't believe this is on DVD. Even less it was available at my local video store.<br /><br />Some argue this is a good movie if you take in consideration it had only a 4000$ budget. I find this funny. I would find it very bad whichever the budget.<br /><br />Still more funny, I read the following in another review: \"Dramatics aside, if you love horror and you love something along the lines of Duel (1971) updated with a little more story and some pretty girls thrown in, you'll love this movie.\"<br /><br />What?!? This is a shame comparing those two movies.<br /><br />I give a \"1\", since I can't give a \"0\". I just don't see any way this movie could be entertaining.\n",
      "\n",
      "Sentiment: pos\n",
      "Text: This was a great movie but it had the worst ending I think I have ever seen!!! The actors were great and displayed wonderful talent. The entire story was twisted and unexpecting, which, is what made it entertaining. As good as the movie was, the entire film is judged by the ending, which was terrible! Maybe a sequel could eliminate this bad ending.\n",
      "\n",
      "Sentiment: neg\n",
      "Text: Richard Dix is a big, not very nice industrialist, who has nearly worked himself to death. If he takes the vacation his doctors suggest for him, can he find happiness for the last months of his life? Well, he'll likely be better off if he disregards the VOICE OF THE WHISTLER.<br /><br />This William Castle directed entry has some great moments (the introduction and the depiction of Richard Dix's life through newsreel a la Citizen Kane), and some intriguing plotting in the final reels. Dix's performance is generally pretty good. But, unfortunately, the just does not quite work because one does not end up buying that the characters would behave the way that they do. Also, the movie veers from a dark (and fascinating beginning) to an almost cheerful 30s movie like midsection (full of nice urban ethnic types who don't mind that they aren't rich) and back again to a complex noir plot for the last 15 minutes or so.<br /><br />This is a decent movie -- worth seeing -- but it needed a little more running time to establish a couple of the characters and a female lead capable of meeting the demands of her role.\n",
      "\n",
      "Sentiment: pos\n",
      "Text: The most interesting thing about Miryang (Secret Sunshine) is the actors. Jeon Do-yeon, as Lee Shin-ae, the main character, is a woman with a young son whose husband has died in a tragic accident, and who leaves Seoul to live in Miryang, which was his home town, with her young son. Jeon's face is very changeable. She is girlish, flirtatious, elegant, aged and sad, desperate and joyous, with it and terribly isolated by turns, and it's all in her face. The film also stars Song Kang-ho as Kim, a man who meets her when her car breaks down coming into Miryang, who happens to run a garage in town, and who follows her around all the time thereafter, despite her apparent lack of interest in his attentions. Song is the biggest star in Korea right now, renowned for his work with Park Chan-wook and Bong Joon-ho (Sympathy for Mr. Vengeance; Memories of Murder and The Host). And yet here he plays a throwaway character, almost a forgotten man. But of course he makes him interesting and curiously appealing. He is the essential ballast to keep Jeon's character from floating away.<br /><br />Lee Shin-ae is a piano teacher. She comes to the new town, which is a neutral place, a kind of poor-man's Seoul, a town \"just like anywhere else,\" as Kim says (just as he is in a way just like anyone else). Her little boy is sprightly, as little boys are, but plainly damaged and withdrawn at times too. His father used to snore, and when he misses him he lies awake, pretending to snore. He goes to school, and Shin-ae meets parents and students and shopkeepers. There is a sense of place in the film, even though the place is in a sense \"anywhere.\" People speak in the local dialect, and everyone knows everything, and Shin-ae's Seoul origin is immediately noticed. Is life really harsher here, away from the big city and its sophistication? Shin-ae seems not to realize the danger she is in.<br /><br />Something terrible happens. And Shin-ae doesn't necessarily deal with it in the best possible way. But it happens and she must face the consequences. But she can't. She goes to pieces. A perpetrator is caught, but that's no consolation. Eventually she becomes so despairing, she relents and goes to a born-again Christian meeting an acquaintance has been pressing her to attend. She finds peace and release with this. But when she decides not only to forgive the perpetrator but to go to the prison to tell him so, that experience is full of ironies and it destroys her all over again. She becomes embittered and desperate and she no longer finds solace in religion. And it gets worse than that.<br /><br />Jeon Do-yeon gives her all in this extremely demanding and protean role. Lee Chang-dong may be a very good director. If an actor of the stature of Song Kang-ho expresses enormous admiration for him, that is convincing. According to Scott Foundas of LA Weekly, Lee's first three films, Green Fish (1997), Peppermint Candy (2000) and Oasis (2002) have marked him out as \"one of the leading figures of his country's recent cinematic renaissance.\" But this is not as successful a film as those of other Korean directors whose work I've seen, such as Yong Sang-Soo, Bong Joon-ho, and the prodigiously, almost perversely gifted Park Chan-wook. It may indeed begin as Foundas says as a kind of \"Asiatic Alice Doesn't Live Here Anymore\" and then \"abruptly and without warning\" turns into \"something of a thriller, and some time after that a nearly Bressonian study in human suffering.\" But that progression not only seems random and indigestible; the film sags and loses its momentum toward the end and then simply fizzles out, with no sense of an ending. There are also weaknesses in the action. Shin-ae takes foolish chances with her son, and makes bad choices all along. If she is destined for madness like Betty in Jean-Jacques Beineix's Betty Blue, which might explain her peculiar and mistaken choices, that isn't something that is properly developed. This is an interesting film, certainly a disturbing one, but one that leaves one doubtful and dissatisfied, after putting one through an emotional wringer.<br /><br />An official selection of the New York Film Festival presented at Lincoln Center, 2007an event that has done right by Korean filmmakers in the recent past.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    label = data_train[i]['sentiment']\n",
    "    txt = data_train[i]['text']\n",
    "    print(f'Sentiment: {label}')\n",
    "    print(f'Text: {txt}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum length of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12424,
     "status": "ok",
     "timestamp": 1573123244270,
     "user": {
      "displayName": "Javier Fernandez",
      "photoUrl": "",
      "userId": "14768449372096605825"
     },
     "user_tz": -540
    },
    "id": "n5XxDW93mhA9",
    "outputId": "4a076ee8-0897-4c87-c42b-11307ba7f062",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470\n"
     ]
    }
   ],
   "source": [
    "lens = [len(d['text'].split()) for d in data_train]\n",
    "print(np.max(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually getting into the classification, let's do a quite recap of one of the most important embedding methods in NLP: Word2Vec.\n",
    "\n",
    "Links:\n",
    "1. https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/\n",
    "2. https://radimrehurek.com/gensim/models/word2vec.html\n",
    "3. https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook\n",
    "4. Word2Vec from scratch: https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "5. Word2Vec with Gensim: https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "\n",
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 4.31084063734327\n",
      "Loss at epo 10: 3.9481004919324603\n",
      "Loss at epo 20: 3.705983177253178\n",
      "Loss at epo 30: 3.524117525986263\n",
      "Loss at epo 40: 3.3792495233672004\n",
      "Loss at epo 50: 3.259489371095385\n",
      "Loss at epo 60: 3.1575867005756924\n",
      "Loss at epo 70: 3.068761876651219\n",
      "Loss at epo 80: 2.989760562351772\n",
      "Loss at epo 90: 2.918317857810429\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 15])\n",
      "torch.Size([15, 5])\n"
     ]
    }
   ],
   "source": [
    "print(W1.shape)\n",
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting question is: Which weight should we use?\n",
    "\n",
    "The answer is W1. Here are some links though that discuss about this topic:\n",
    "\n",
    "1. They both capture the word semantics. Not only W, sometimes W' is also used as word vectors. Even in somecases (W+W')/2 has also been used and better results in that particular task have been obtained.\n",
    "2. The output context matrix encodes the meanings of words as context, different from the embedding matrix . NOTE: Despite the name, is independent of , not a transpose or inverse or whatsoever. (https://lilianweng.github.io/posts/2017-10-15-word-embedding/)\n",
    "3. https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word\n",
    "4. Building and training was fun and all, but our end goal was not to build a neural network; we wanted to get word embeddings. As stated earlier in this post, the key behind word embeddings is that the rows of the first weight matrix is effectively a dense representation of one-hot encoded vectors each corresponding to various tokens in the text dataset (https://jaketae.github.io/study/word2vec/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short version of the implementation of Word2Vec with gensim would be the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataset_to_tensors(dataset):\n",
    "    sentiments, embeddings = [], []\n",
    "    for d in dataset:\n",
    "        wordvecs = d['text']\n",
    "        embeddings.append(wordvecs)\n",
    "    return embeddings\n",
    "\n",
    "train_data = dataset_to_tensors(data_train)\n",
    "test_data = dataset_to_tensors(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(train_data, vector_size=100, window=5, min_count=5, workers=4)\n",
    "weights = torch.FloatTensor(model.wv.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPEmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use another type of embedding for the posterior classification: BPEmb\n",
    "\n",
    "This is a set of pre-trained embeddings that we will use to convert the text into a sequence of vectors for subsequent analysis. The idea is to extract labels from the dataset, and parse text into a set of subword vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "673\n",
      "(210, 50)\n",
      "(194, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train[0]['text']))\n",
    "print(len(data_train[1]['text']))\n",
    "\n",
    "bpe = BPEmb(lang='en', dim=50)\n",
    "print(bpe.embed(data_train[0]['text']).shape)\n",
    "print(bpe.embed(data_train[1]['text']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RxgnwW1Jdagv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataset_to_tensors(dataset):\n",
    "    sentiments, embeddings = [], []\n",
    "    bpe = BPEmb(lang='en', dim=50)\n",
    "    for d in dataset:\n",
    "        if d['sentiment'] == 'pos':\n",
    "            sentiments.append(1)\n",
    "        else:\n",
    "            sentiments.append(0)\n",
    "      \n",
    "        wordvecs = bpe.embed(d['text'])\n",
    "        embeddings.append(wordvecs)\n",
    "    return np.array(sentiments), embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46881,
     "status": "ok",
     "timestamp": 1573093264287,
     "user": {
      "displayName": "Nicholas Guttenberg",
      "photoUrl": "",
      "userId": "06849451791642134997"
     },
     "user_tz": -540
    },
    "id": "sV1JnrV7c1PE",
    "outputId": "769c8f45-51d0-427e-825d-2b98864963eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_labels, train_tensors = dataset_to_tensors(data_train)\n",
    "test_labels, test_tensors = dataset_to_tensors(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([12500, 12500]))\n",
      "(array([0, 1]), array([12500, 12500]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what one of our inputs will look like\n",
    "\n",
    "Note that each one will have a different sequence length, which will make batching difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnqzMBHAeTc3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.271187  0.202276 -0.371739 ... -0.628708  0.477076  0.206391]\n",
      " [-0.341687  0.078269  0.315032 ...  0.242251 -0.149144  0.422889]\n",
      " [-0.439638 -0.303795  0.780145 ... -0.610518 -0.001575  0.187587]\n",
      " ...\n",
      " [ 0.727799  0.067326  0.168493 ...  0.307808  0.1344   -0.206341]\n",
      " [-1.273908 -0.607935  0.375018 ...  0.364744 -0.068366  0.425233]\n",
      " [-0.275946  0.033621 -0.175133 ... -0.063625  0.111334  0.193656]]\n",
      "(210, 50)\n",
      "[[-0.271187  0.202276 -0.371739 ... -0.628708  0.477076  0.206391]\n",
      " [-0.023115 -0.254144  0.209979 ... -0.279852 -0.692482  0.9685  ]\n",
      " [ 0.017189  0.269219  0.051473 ...  0.24861   0.451184 -0.059601]\n",
      " ...\n",
      " [-0.298372 -0.09406   0.25116  ... -0.51283  -0.16148   0.690044]\n",
      " [-0.466095  0.107843  0.310546 ... -0.009862 -0.057548  0.435935]\n",
      " [-0.275946  0.033621 -0.175133 ... -0.063625  0.111334  0.193656]]\n",
      "(194, 50)\n",
      "Maximum length:  3616\n",
      "Mean length:  345.2042\n"
     ]
    }
   ],
   "source": [
    "print(train_tensors[0])\n",
    "print(train_tensors[0].shape)\n",
    "\n",
    "print(train_tensors[1])\n",
    "print(train_tensors[1].shape)\n",
    "\n",
    "print(\"Maximum length: \",np.max([d.shape[0] for d in train_tensors]))\n",
    "print(\"Mean length: \", np.mean([d.shape[0] for d in train_tensors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a function to assemble a batch from a dataset, such that everything is padded with zero vectors to the longest example in the batch\n",
    "Inside the network, we're going to use a function to only take the sequence processed up to the final point for each case.\n",
    "So this should only produce overhead, not change the algorithm\n",
    " \n",
    "Since the reviews are very long, we'll clip them after a certain number of tokens (variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU57BX-zfiyZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assemble_batch(x, y, batchidx, BS = 20, clip=500):\n",
    "    labels = []\n",
    "    tensors = []\n",
    "    lens = [np.minimum(d.shape[0],clip) for d in x[BS*batchidx:BS*(batchidx+1)]]\n",
    "\n",
    "    maxlen = np.max(lens)\n",
    "\n",
    "    labels = y[BS*batchidx:BS*(batchidx+1)]\n",
    "\n",
    "    for i in range(batchidx*BS, (batchidx+1)*BS):\n",
    "        if x[i].shape[0]<maxlen:\n",
    "            d = np.concatenate([x[i], np.zeros((maxlen-x[i].shape[0], 50))], axis=0)\n",
    "        else:\n",
    "            d = x[i][:maxlen]\n",
    "        tensors.append(d)\n",
    "    return np.array(tensors), labels, np.array(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is in the format batchdim x sequence x features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tH7hVYqVhPNn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 500, 50) (20,) (20,)\n"
     ]
    }
   ],
   "source": [
    "b_x, b_y, b_l = assemble_batch(train_tensors, train_labels, 0)\n",
    "print(b_x.shape, b_y.shape, b_l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets try just aggregating sentiment predictions for each component word-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2rl_6FhnBxf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BagNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Conv1d(50,128,1)\n",
    "        self.l2 = nn.Conv1d(128,128,1)\n",
    "        self.l3 = nn.Conv1d(128,128,1)\n",
    "        self.drop1 = nn.Dropout(p=0.05)\n",
    "        self.l4 = nn.Linear(128,128)\n",
    "        self.drop2 = nn.Dropout(p=0.05)\n",
    "        self.l5 = nn.Linear(128,1)\n",
    "        self.optim = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        z = x.transpose(1,2).contiguous()\n",
    "        z = F.relu(self.l1(z))\n",
    "        z = F.relu(self.l2(z))\n",
    "        z = self.l3(z)\n",
    "        z = z.sum(2)\n",
    "        z = z/lengths.float().unsqueeze(1)\n",
    "        z = self.drop1(z)\n",
    "        z = self.drop2(F.relu(self.l4(z)))\n",
    "        z = torch.sigmoid(self.l5(z))[:,0]\n",
    "        return z\n",
    "\n",
    "def predict(text, model):\n",
    "    bpe = BPEmb(lang='en', dim=50)\n",
    "    vecs = bpe.embed(text) # sequence * features\n",
    "    x = torch.cuda.FloatTensor(vecs).unsqueeze(0) # 1 * sequence * features\n",
    "    l = torch.cuda.LongTensor(np.array([x.size(1)]))\n",
    "    p = model.forward(x,l)\n",
    "    return p.cpu().detach().item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSB__bdnnk_x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bagnet = BagNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model\n",
    " \n",
    "Batch size seems to matter for this model - I lost 7% accuracy with batch size of 100 versus batch size of 200.\n",
    "Somewhat unusual..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eval() changes the bn and dropout layer’s behaviour\n",
    "- torch.no_grad() deals with the autograd engine and stops it from calculating the gradients, which is the recommended way of doing validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGhsC4K1nnca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BS = 10\n",
    "train_error, test_error = [], []\n",
    "train_accuracy, test_accuracy = [], []\n",
    "for epoch in range(10):  \n",
    "    # Training loop\n",
    "    errs, accs = [], []\n",
    "    bagnet.train()\n",
    "    for i in range(train_labels.shape[0]//BS):\n",
    "        bagnet.optim.zero_grad()\n",
    "        x, y, l = assemble_batch(train_tensors, train_labels, i, BS, clip=500)\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "        y = torch.FloatTensor(y).to(device)\n",
    "        l = torch.LongTensor(l).to(device)\n",
    "        p = bagnet.forward(x,l)\n",
    "\n",
    "        # Log loss, with fudge factors to prevent NaN\n",
    "        loss = -torch.mean(y*torch.log(p+1e-8) + (1-y)*torch.log(1-p+1e-8))\n",
    "        acc = torch.mean(torch.ge(p,0.5).float()*y + (1-torch.ge(p,0.5).float())*(1-y))\n",
    "        loss.backward()\n",
    "        bagnet.optim.step()\n",
    "\n",
    "        # Accumulate stats\n",
    "        errs.append(loss.cpu().detach().item())\n",
    "        accs.append(acc.cpu().detach().item())\n",
    "        \n",
    "        break\n",
    "    train_error.append(np.mean(errs))\n",
    "    train_accuracy.append(np.mean(accs))\n",
    "\n",
    "    # Testing loop\n",
    "    errs, accs = [], []\n",
    "    bagnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_labels.shape[0]//BS):\n",
    "            x, y, l = assemble_batch(test_tensors, test_labels, i, BS)\n",
    "            x = torch.FloatTensor(x)\n",
    "            y = torch.FloatTensor(y)\n",
    "            l = torch.LongTensor(l)\n",
    "            p = bagnet.forward(x, l)\n",
    "\n",
    "            # Log loss, with fudge factors to prevent NaN\n",
    "            loss = -torch.mean(y*torch.log(p+1e-8) + (1-y)*torch.log(1-p+1e-8))\n",
    "            acc = torch.mean(torch.ge(p,0.5).float()*y + (1-torch.ge(p,0.5).float())*(1-y))\n",
    "\n",
    "            # Accumulate stats\n",
    "            errs.append(loss.cpu().detach().item())\n",
    "            accs.append(acc.cpu().detach().item())\n",
    "  \n",
    "    test_error.append(np.mean(errs))\n",
    "    test_accuracy.append(np.mean(accs))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Log loss\")\n",
    "    plt.plot(train_error, label=\"Train\")\n",
    "    plt.plot(test_error, label=\"Test\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accuracy, label=\"Train\")\n",
    "    plt.plot(test_accuracy, label=\"Test\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    plt.gcf().set_size_inches((8,4))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pN6cRjuY7SOz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Final accuracy: \", test_accuracy[-1])\n",
    "\n",
    "print(review1)\n",
    "print(\"Review 1 prediction: \", predict(review1, bagnet))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(review2)\n",
    "print(\"Review 2 prediction: \", predict(review2, bagnet))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(review3)\n",
    "print(\"Review 3 prediction: \", predict(review3, bagnet))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an LSTM to classify sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhmPyHxthVe9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # This can overfit quite easily, so use dropout on the embeddings\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "\n",
    "        # The output will be of the form batch x sequence x features. However, we're just going to use the final element for the classifier.\n",
    "        # Using a very tiny LSTM because otherwise it overfits    \n",
    "        self.lstm = nn.LSTM(50, 50, batch_first=True)\n",
    "\n",
    "        self.classify_layer = nn.Linear(50, 1)\n",
    "\n",
    "        # For LSTMs, we want a fairly high learning rate if we can get away with it\n",
    "        self.optim = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x is Batch x Sequence x Features\n",
    "        z = self.drop(x)\n",
    "\n",
    "        # We don't need the hidden state or cell state outputs, so discard them   \n",
    "        z, _ = self.lstm(z)\n",
    "\n",
    "        # Take the LSTM state after the last token for each sentence, ignoring parts after the end\n",
    "        idx = torch.arange(x.size(0)).long().cuda()\n",
    "        l = lengths - 1\n",
    "        z = z[idx, l[idx]]\n",
    "        z = torch.sigmoid(self.classify_layer(z))\n",
    "\n",
    "        return z[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlQSstlci1ET",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTwxQOVDi13h",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now train the model\n",
    "BS = 200\n",
    "train_error, test_error = [], []\n",
    "train_accuracy, test_accuracy = [], []\n",
    "\n",
    "for epoch in range(30):\n",
    "    # Training loop\n",
    "    errs = []\n",
    "    accs = []\n",
    "    net.train()\n",
    "    for i in range(train_labels.shape[0]//BS):\n",
    "        net.optim.zero_grad()\n",
    "        x,y,l = assemble_batch(train_tensors, train_labels, i, BS)\n",
    "        x = torch.cuda.FloatTensor(x)\n",
    "        y = torch.cuda.FloatTensor(y)\n",
    "        l = torch.cuda.LongTensor(l)\n",
    "\n",
    "        p = net.forward(x,l)\n",
    "\n",
    "        # Log loss, with fudge factors to prevent NaN\n",
    "        loss = -torch.mean(y*torch.log(p+1e-8) + (1-y)*torch.log(1-p+1e-8))\n",
    "        acc = torch.mean(torch.ge(p,0.5).float()*y + (1-torch.ge(p,0.5).float())*(1-y))\n",
    "        loss.backward()\n",
    "        net.optim.step()\n",
    "\n",
    "        # Accumulate stats\n",
    "        errs.append(loss.cpu().detach().item())\n",
    "        accs.append(acc.cpu().detach().item())\n",
    "  \n",
    "    train_error.append(np.mean(errs))\n",
    "    train_accuracy.append(np.mean(accs))\n",
    "\n",
    "    # Testing loop\n",
    "    errs = []\n",
    "    accs = []\n",
    "    net.eval()\n",
    "    for i in range(test_labels.shape[0]//BS):\n",
    "        x,y,l = assemble_batch(test_tensors, test_labels, i, BS)\n",
    "        x = torch.cuda.FloatTensor(x)\n",
    "        y = torch.cuda.FloatTensor(y)\n",
    "        l = torch.cuda.LongTensor(l)\n",
    "\n",
    "        p = net.forward(x,l)\n",
    "\n",
    "        # Log loss, with fudge factors to prevent NaN\n",
    "        loss = -torch.mean(y*torch.log(p+1e-8) + (1-y)*torch.log(1-p+1e-8))\n",
    "        acc = torch.mean(torch.ge(p,0.5).float()*y + (1-torch.ge(p,0.5).float())*(1-y))\n",
    "\n",
    "        # Accumulate stats\n",
    "        errs.append(loss.cpu().detach().item())\n",
    "        accs.append(acc.cpu().detach().item())\n",
    "  \n",
    "    test_error.append(np.mean(errs))\n",
    "    test_accuracy.append(np.mean(accs))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Log loss\")\n",
    "    plt.plot(train_error, label=\"Train\")\n",
    "    plt.plot(test_error, label=\"Test\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accuracy, label=\"Train\")\n",
    "    plt.plot(test_accuracy, label=\"Test\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    plt.gcf().set_size_inches((8,4))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYQIDBmvkIKp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Final accuracy: \", test_accuracy[-1])\n",
    "\n",
    "print(review1)\n",
    "print(\"Review 1 prediction: \", predict(review1, net))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(review2)\n",
    "print(\"Review 2 prediction: \", predict(review2, net))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(review3)\n",
    "print(\"Review 3 prediction: \", predict(review3, net))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxvwGG1gLyz4",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLPTutorial.ipynb",
   "provenance": [
    {
     "file_id": "1mL2TYJlF-sK9xwGr2UA-tJyXnkz2j-VD",
     "timestamp": 1573110158333
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
